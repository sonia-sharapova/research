{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df277670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import pytz\n",
    "import datetime\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "# whether or not to split into train and test\n",
    "split_data = True\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "947af769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename, features):\n",
    "    data = read_csv(filename, header=None)\n",
    "    dataset = data.values\n",
    "    labels = dataset[0][features]\n",
    "    \n",
    "    newDataset = dataset[:,features]\n",
    "    newDataset = removeHeader(newDataset)\n",
    "    \n",
    "    X = newDataset[:, :-1]\n",
    "    y = newDataset[:,-1]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def removeHeader(data):\n",
    "    newData = data[1:]\n",
    "    return newData\n",
    "\n",
    "def getTrainTest(dataset, labels):\n",
    "\n",
    "    dataset = dataset.loc[:, labels]\n",
    "\n",
    "    X = dataset.iloc[:, :-1].values\n",
    "    y = dataset.iloc[:, -1].values    # split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, shuffle=False)\n",
    "    features = y\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def convertLabels(dataset, features):\n",
    "    labels = list(dataset)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def predictionsRandomForest(dataset, feats):\n",
    "    \n",
    "    #dataset = pd.read_csv(csv)\n",
    "    \n",
    "    \n",
    "    labels = convertLabels(dataset, feats)\n",
    "    X_train, X_test, y_train, y_test = getTrainTest(dataset, labels)\n",
    "    \n",
    "    intPreds = []\n",
    "    \n",
    "    model = RandomForestRegressor()\n",
    "    #model.fit(X_train, y_train)\n",
    "    #y_pred = model.predict(X_test)\n",
    "    \n",
    "    \"\"\"starts tutorial\"\"\"\n",
    "    \n",
    "    # Saving feature names for later use\n",
    "    feature_list = list(dataset)\n",
    "    # The baseline predictions are the historical averages\n",
    "    baseline_preds = X_test[:,0]\n",
    "    #print(baseline_preds)\n",
    "    # Baseline errors, and display average baseline error\n",
    "    baseline_errors = abs(baseline_preds.astype(int) - y_test.astype(int))\n",
    "    #print('Average baseline error: ', round(np.mean(baseline_errors), 2))\n",
    "    #Average baseline error:  5.06 degrees.\n",
    "    \n",
    "    # Train the model on training data\n",
    "    model.fit(X_train, y_train);\n",
    "    # Use the forest's predict method on the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Calculate the absolute errors\n",
    "    errors = abs(y_pred - y_test)\n",
    "    # Print out the mean absolute error (mae)\n",
    "    mae = round(np.mean(errors), 2)\n",
    "    #print('Mean Absolute Error:', mae, 'degrees.')\n",
    "    \n",
    "    \n",
    "    for x in y_pred:\n",
    "        intPreds.append(round(x))\n",
    "                \n",
    "    normPred = np.linalg.norm(intPreds - np.array(y_test))\n",
    "    corr = correlation(intPreds, y_test)\n",
    "    return(mae, y_test)\n",
    "    #return(normPred, y_test, corr)\n",
    "\n",
    "\n",
    "def predictionsLinearRegression(dataset, feats):\n",
    "    \n",
    "    #dataset = pd.read_csv(csv)\n",
    "    \n",
    "    labels = convertLabels(dataset, feats)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = getTrainTest(dataset, labels)\n",
    "\n",
    "    intPreds = []\n",
    "    model = LinearRegression()\n",
    "    \"\"\"model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\"\"\"\n",
    "    \n",
    "    #tutorial\n",
    "    feature_list = list(dataset)\n",
    "    baseline_preds = X_test[:,0]\n",
    "    baseline_errors = abs(baseline_preds - y_test)\n",
    "    \n",
    "    model.fit(X_train, y_train);\n",
    "    y_pred = model.predict(X_test)\n",
    "    errors = abs(y_pred - y_test)\n",
    "    mae = round(np.mean(errors), 2)\n",
    "    \n",
    "    \n",
    "    for x in y_pred:\n",
    "        intPreds.append(round(x))\n",
    "                \n",
    "    normPred = np.linalg.norm(intPreds - np.array(y_test))\n",
    "    corr = correlation(intPreds, y_test)\n",
    "    \n",
    "    return(mae, y_test)\n",
    "\n",
    "    #return(normPred, y_test, corr)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "def predictionsSVM(dataset, feats):\n",
    "    print(feats)\n",
    "    #dataset = pd.read_csv(csv)\n",
    "    labels = convertLabels(dataset, feats)\n",
    "    X_train, X_test, y_train, y_test = getTrainTest(dataset, labels)\n",
    "    intPreds = []\n",
    "    \n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "    lab_enc = preprocessing.LabelEncoder()\n",
    "    encoded = lab_enc.fit_transform(y_train)\n",
    "    \n",
    "    model = svm.SVC()\n",
    "    \"\"\"model.fit(X_train, encoded)\n",
    "    #model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\"\"\"\n",
    "    \n",
    "    #tutorial\n",
    "    feature_list = list(dataset)\n",
    "    baseline_preds = X_test[:,0]\n",
    "    baseline_errors = abs(baseline_preds - y_test)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    errors = abs(y_pred - y_test)\n",
    "    mae = round(np.mean(errors), 2)\n",
    "    \n",
    "    for x in y_pred:\n",
    "        intPreds.append(round(x))\n",
    "                \n",
    "    normPred = np.linalg.norm(intPreds - np.array(y_test))\n",
    "    corr = correlation(intPreds, y_test)\n",
    "    \n",
    "    return(mae, y_test)\n",
    "\n",
    "    #return(normPred, y_test, corr)\n",
    "\n",
    "\n",
    "def predictionsLASSO(dataset, feats):\n",
    "    \n",
    "    #dataset = pd.read_csv(csv)\n",
    "    labels = convertLabels(dataset, feats)\n",
    "    X_train, X_test, y_train, y_test = getTrainTest(dataset, labels)\n",
    "    intPreds = []\n",
    "    \n",
    "    model = Lasso(alpha=1.0)\n",
    "    \"\"\"model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\"\"\"\n",
    "    \n",
    "    \n",
    "    #tutorial\n",
    "    feature_list = list(dataset)\n",
    "    baseline_preds = X_test[:,0]\n",
    "    baseline_errors = abs(baseline_preds - y_test)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    errors = abs(y_pred - y_test)\n",
    "    mae = round(np.mean(errors), 2)\n",
    "    \n",
    "    \n",
    "    for x in y_pred:\n",
    "        intPreds.append(round(x))\n",
    "                \n",
    "    normPred = np.linalg.norm(intPreds - np.array(y_test))\n",
    "    corr = correlation(intPreds, y_test)\n",
    "    #return(normPred, y_test, corr)\n",
    "    return(mae, y_test)\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def predictionsGradientBoostingClassifier(dataset, feats):\n",
    "    \n",
    "    \n",
    "    #dataset = pd.read_csv(csv)\n",
    "    \n",
    "    labels = convertLabels(dataset, feats)\n",
    "    X_train, X_test, y_train, y_test = getTrainTest(dataset, labels)\n",
    "    \n",
    "    intPreds = []\n",
    "    \n",
    "    lab_enc = preprocessing.LabelEncoder()\n",
    "    encoded = lab_enc.fit_transform(y_train)\n",
    "    \n",
    "    model = GradientBoostingClassifier()\n",
    "    model.fit(X_train, encoded)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    for x in y_pred:\n",
    "        intPreds.append(round(x))\n",
    "                \n",
    "    normPred = np.linalg.norm(intPreds - np.array(y_test))\n",
    "    corr = correlation(intPreds, y_test)\n",
    "    return(normPred, y_test, corr)\n",
    "    \n",
    "    \n",
    "#def bestPred(features, predictions, numFeatsToRem):\n",
    "    \n",
    "\n",
    "def remWorstFeat(features, predictions, numFeatsToRem):\n",
    "    worst = 0\n",
    "    length = len(predictions)\n",
    "    assert(len(features)==len(predictions))\n",
    "        \n",
    "    for x in range(numFeatsToRem):\n",
    "        worst = 0 #index of worst feature\n",
    "        for y in range(length):\n",
    "            if(predictions[y] > predictions[worst]):\n",
    "                worst = y\n",
    "        \n",
    "        if (length > 1):\n",
    "            #print(predictions, predictions[worst])\n",
    "            #print(features, features[worst])\n",
    "        \n",
    "            del predictions[worst]\n",
    "            del features[worst]\n",
    "        length = length - 1\n",
    "    return features\n",
    "\n",
    "    \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def correlation(y_pred, y_test):\n",
    "    covariance = np.cov(y_pred, y_test)\n",
    "    pred = ['y_pred', 'y_test']\n",
    "    \n",
    "    correlation = np.corrcoef(y_pred, y_test)\n",
    "    return(correlation[0][1])\n",
    "\n",
    "\n",
    "def correlationArray(correlation):\n",
    "    return correlation[0][1]\n",
    "    \n",
    "import copy\n",
    "def bestFeatsFirstCopy(dataset, feats, model):\n",
    "    featurePred=[]\n",
    "    featureCorrs=[]\n",
    "    \n",
    "    sublength = len(feats) #length of feature list post removal\n",
    "    featsCopy = copy.deepcopy(feats) # array of features post removal\n",
    "    featSubset = copy.deepcopy(feats) #for loop feature (remove one at a time)\n",
    "    allCorrIters = [] #append each array, removing one at a time\n",
    "    \n",
    "\n",
    "    run = 0\n",
    "    #without while loop (one iteration)\n",
    "    \"\"\"for x in range(length-1):\n",
    "        featsCopy = featsCopy[:x] + featsCopy[(x+1):]\n",
    "        predict = predictions(csv, featsCopy)\n",
    "        featurePred.append(predict[0])\n",
    "        featureCorrs.append(predict[2])\n",
    "        featsCopy = []\n",
    "        featsCopy = feats   \"\"\"\n",
    "        \n",
    "    #with while loop (many iterations to find best)\n",
    "    #while sublength != 2: #2 best features\n",
    "    assert(sublength > 10)\n",
    "    \n",
    "    while sublength != 10: #10 best features\n",
    "        for x in range(len(featsCopy) - 1):\n",
    "            featSubset = featSubset[:x] + featSubset[(x+1):]\n",
    "            if(model == 'RandomForest'):\n",
    "                predict = predictionsRandomForest(csv, featSubset)\n",
    "            elif(model == 'LinearRegression'):\n",
    "                predict = predictionsLinearRegression(csv, featSubset)\n",
    "            elif(model == 'SVM'):\n",
    "                predict = predictionsSVM(csv, featSubset)\n",
    "            elif(model == 'LASSO'):\n",
    "                predict = predictionsLASSO(csv, featSubset)\n",
    "            elif(model == 'GradientBoostingClassifier'):\n",
    "                predict = predictionsGradientBoostingClassifier(csv, featSubset)\n",
    "            #featurePred.append(predict[0])\n",
    "            featureCorrs.append(predict[2])\n",
    "            featSubset = copy.deepcopy(featsCopy)\n",
    "            \n",
    "        run = run + 1\n",
    "        print(\"run = \")\n",
    "        print(run)\n",
    "        #print(featureCorrs)\n",
    "        allCorrIters.append(featureCorrs)\n",
    "        if sublength >= 50:\n",
    "            featsCopy = remWorstFeat(featsCopy[:-1], featureCorrs, 50)\n",
    "        else:\n",
    "            featsCopy = remWorstFeat(featsCopy[:-1], featureCorrs, 1)\n",
    "        #remWorstFeat should return feature list\n",
    "        sublength = len(featsCopy)\n",
    "        featSubset = copy.deepcopy(featsCopy)\n",
    "        featureCorrs = []\n",
    "        \n",
    "    \n",
    "    \n",
    "    featureCorrs = allCorrIters[-1]\n",
    "    print(featureCorrs)\n",
    "    \n",
    "    \"\"\"print(\"Prediction\")\n",
    "    print(featurePred)\n",
    "    print(predict[1])\"\"\"\n",
    "\n",
    "    #featurePred shows the prediction if this feature was removed. If score is low, this means prediction would be lower if this was removed\n",
    "    \n",
    "    \"\"\" L = [ (featurePred[i],i) for i in range(len(featurePred)) ]\n",
    "    L.sort()\n",
    "    sorted_l,permutationL = zip(*L)\n",
    "    permutationNpL = np.array(permutationL)\n",
    "    print(permutationNpL)\n",
    "    print(convertLabels(csv, permutationNpL))\"\"\"\n",
    "    #featureCorrs shows the prediction if this feature was removed. If score is low, this means prediction would be lower if this was removed\n",
    "\n",
    "    \n",
    "    \n",
    "    print(\"Correlation\")\n",
    "    print(featureCorrs)\n",
    "\n",
    "    N = [ (featureCorrs[j],j) for j in range(len(featureCorrs)) ]\n",
    "    N.sort()\n",
    "    sorted_n,permutationN = zip(*N)\n",
    "    permutationNpN = np.array(permutationN)\n",
    "        \n",
    "    print(permutationNpN)\n",
    "    print(convertLabels(csv, permutationNpN))\n",
    "\n",
    "    #featurePred = np.array(featurePred)\n",
    "    featureCorrs = np.array(featureCorrs)\n",
    "\n",
    "    corrVar = np.var(featureCorrs)\n",
    "    print(\"variance: \", corrVar)\n",
    "    \n",
    "    #return permutationNpL, permutationNpN\n",
    "    #return permutationNpN\n",
    "    return corrVar\n",
    "\n",
    "\n",
    "\n",
    "def bestFeats(dataset, feats, model):\n",
    "    bestFeatures=[]\n",
    "    bestPreds=[]\n",
    "    remainingFeatures=copy.deepcopy(feats)\n",
    "    workingSubset = np.empty((10)).astype(np.int32) #optimize over\n",
    "    #predictions=np.empty((len(feats))).astype(np.int32)\n",
    "    predictions=np.empty((len(feats)))\n",
    "    \n",
    "    while len(bestFeatures) < 10:\n",
    "        for x in range(len(remainingFeatures)-1):\n",
    "            #subset always includes best features, loop over remaining\n",
    "            for y in range(len(bestFeatures)):\n",
    "                workingSubset[y] = feats[bestFeatures[y]]\n",
    "            \n",
    "            workingSubset[len(bestFeatures)] = feats[remainingFeatures[x]]\n",
    "            #workingSubset[len(bestFeatures)+1] = feats[-1]\n",
    "\n",
    "            #print(workingSubset)\n",
    "            if(model == 'RandomForest'):\n",
    "                predictions[x]=predictionsRandomForest(dataset, workingSubset[:len(bestFeatures)+1])[0]\n",
    "                #print('prediction',predictions[x])\n",
    "            elif(model == 'LinearRegression'):\n",
    "                predictions[x] = predictionsLinearRegression(dataset, workingSubset[:len(bestFeatures)+1])[0]\n",
    "            elif(model == 'SVM'):\n",
    "                predictions[x] = predictionsSVM(dataset, workingSubset[:len(bestFeatures)+1])[0]\n",
    "            elif(model == 'LASSO'):\n",
    "                predictions[x] = predictionsLASSO(dataset, workingSubset[:len(bestFeatures)+1])[0]\n",
    "            elif(model == 'GradientBoostingClassifier'):\n",
    "                predictions[x] = predictionsGradientBoostingClassifier(dataset, workingSubset[:len(bestFeatures)+1])[0]\n",
    "                \n",
    "            \n",
    "        #smallest error is best\n",
    "        \n",
    "        #xx = predictions[:len(remainingFeatures)].argmax() #x corresponding to best x in remaining feature list\n",
    "        minimum = predictions[:len(remainingFeatures)].argmin()\n",
    "        xx = remainingFeatures[minimum]\n",
    "\n",
    "        bestPreds.append(predictions[xx])\n",
    "        #bestFeatures.append(remainingFeatures[xx])\n",
    "        bestFeatures.append(remainingFeatures[xx])\n",
    "        \n",
    "        del remainingFeatures[xx]\n",
    "        print('run', len(bestFeatures) + 1)\n",
    "        \n",
    "        #find location in feats\n",
    "    \n",
    "    ########\n",
    "    \n",
    "    print(bestFeatures)\n",
    "    print(bestPreds)\n",
    "       \n",
    "    return bestFeatures, bestPreds\n",
    "\n",
    "\n",
    "\n",
    "def createList(n, remFirstN):\n",
    "    lst = []\n",
    "    for i in range(n):\n",
    "        if i >(remFirstN - 1):\n",
    "            lst.append(i)\n",
    "    return(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8713a2e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/soniasharapova/Documents/JunDingResearch/MultiOmics/methylation/c9e2bffb-c8f2-438e-8838-0889a9043d3e.methylation_array.sesame.level3betas.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-3c2682a6d394>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdatasetRnaSeq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/soniasharapova/Documents/JunDingResearch/MultiOmics/rnaSeq/2056c543-2f73-4a2f-995e-3af2dda7b514.rna_seq.augmented_star_gene_counts.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdatasetMirnaSeq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/soniasharapova/Documents/JunDingResearch/MultiOmics/miRnaSeq/827ff7db-cd52-431b-98b9-54c39edc9bf6.mirbase21.mirnas.quantification.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdatasetMethylation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/soniasharapova/Documents/JunDingResearch/MultiOmics/methylation/c9e2bffb-c8f2-438e-8838-0889a9043d3e.methylation_array.sesame.level3betas.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/soniasharapova/Documents/JunDingResearch/MultiOmics/methylation/c9e2bffb-c8f2-438e-8838-0889a9043d3e.methylation_array.sesame.level3betas.csv'"
     ]
    }
   ],
   "source": [
    "datasetRnaSeq = pd.read_csv('/Users/soniasharapova/Documents/JunDingResearch/MultiOmics/rnaSeq/2056c543-2f73-4a2f-995e-3af2dda7b514.rna_seq.augmented_star_gene_counts.csv')\n",
    "datasetMirnaSeq = pd.read_csv('/Users/soniasharapova/Documents/JunDingResearch/MultiOmics/miRnaSeq/827ff7db-cd52-431b-98b9-54c39edc9bf6.mirbase21.mirnas.quantification.csv')\n",
    "datasetMethylation = pd.read_csv('/Users/soniasharapova/Documents/JunDingResearch/MultiOmics/methylation/c9e2bffb-c8f2-438e-8838-0889a9043d3e.methylation_array.sesame.level3betas.csv')\n",
    "\n",
    "\n",
    "rnaVars = []\n",
    "mirnaVars = []\n",
    "methylVars = []\n",
    "\n",
    "print(datasetRnaSeq)\n",
    "print(datasetMirnaSeq)\n",
    "print(datasetMethylation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72636b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 5]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-ad9d0013a34a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#dataset = pd.read_csv('/Users/soniasharapova/Documents/MultiOmics/mirnaCategorical.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnaFeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrnaRandomForestVar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbestFeats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasetRnaSeq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnaFeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'RandomForest'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#mirnaRandomForestVar = bestFeats('/Users/soniasharapova/Documents/MultiOmics/mirnaCategorical.csv', mirnaFeatures, 'RandomForest')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mrnaVars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RandomForest'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnaRandomForestVar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-aab68626da3e>\u001b[0m in \u001b[0;36mbestFeats\u001b[0;34m(dataset, feats, model)\u001b[0m\n\u001b[1;32m    367\u001b[0m                 \u001b[0mworkingSubset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbestFeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m             \u001b[0mworkingSubset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbestFeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mremainingFeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m             \u001b[0;31m#workingSubset[len(bestFeatures)+1] = feats[-1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "rnaFeatures = createList(6, 3)\n",
    "#dataset = pd.read_csv('/Users/soniasharapova/Documents/MultiOmics/mirnaCategorical.csv')\n",
    "print(rnaFeatures)\n",
    "rnaRandomForestVar = bestFeats(datasetRnaSeq, rnaFeatures, 'RandomForest')[1]\n",
    "#mirnaRandomForestVar = bestFeats('/Users/soniasharapova/Documents/MultiOmics/mirnaCategorical.csv', mirnaFeatures, 'RandomForest')\n",
    "rnaVars.append(['RandomForest', rnaRandomForestVar])\n",
    "print(\"-------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7970a2df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
